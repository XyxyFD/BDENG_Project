{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dbe5e28-4961-4e3e-905e-e60cc1a5c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.11/site-packages (2.2.13)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.11/site-packages (0.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456016ff-636a-490e-9a91-34fa2fe6fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "from kafka import KafkaProducer, KafkaConsumer \n",
    "from kafka import KafkaAdminClient\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, avg\n",
    "from pyspark.sql.types import StructType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb9d0f3-c6a7-4a83-aa54-dcee5383c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktuelle Topics: ['big-data-test', 'github-trending-all', 'github-scraped-trending', 'hello-world', 'steam-pc-prices', 'flightsTest', 'srp-2022-data', 'energy-consumption2', 'stocks', 'eu_energy_data', 'music', 'roulette', 'github-trending-all-v1', 'g3-hello-world', 'github-trending-all-v3', 'geizhals-ssd', 'github-trending-all-v2', 'titanic-stream', 'steam-hwsurvey-summary', 'gpu-topic', 'amadeus_flights', 'current-weather-api', 'wikimedia-changes', 'intraday-data', 'geizhals-gpu', 'srp-data', 'energy-sustainability', 'taxi_samples', 'energy-consumption', 'g3-raw-html-test', 'weather-report', 'geizhals-cpu', 'finanzdaten', 'flights', 'geizhals-ram', '__consumer_offsets', 'music_data']\n"
     ]
    }
   ],
   "source": [
    "admin = KafkaAdminClient(bootstrap_servers=\"172.29.16.101:9092\")\n",
    "print(\"Aktuelle Topics:\", admin.list_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee977cc-d494-47a0-aece-29da32de01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = ['BER', 'CDG', 'IST', 'LHR']\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "for airport in airports:\n",
    "    df = pd.read_csv(f\"Kafka_Spark/CSVs/amadeus_prices_{airport}.csv\")\n",
    "    df[\"Abflug\"] = airport\n",
    "    df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "df_all['FetchedAt'] = pd.to_datetime(df_all['FetchedAt'], errors='coerce')\n",
    "df_all.dropna(inplace=True)\n",
    "df_all['FetchedAt'] = df_all['FetchedAt'].dt.strftime('%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f074b340-b218-4d8f-93f8-8cc2e06a4824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten wurden an Kafka gesendet.\n"
     ]
    }
   ],
   "source": [
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=\"172.29.16.101:9092\",\n",
    "    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
    ")\n",
    "for _, row in df_all.iterrows():\n",
    "    producer.send('amadeus_flights', row.to_dict())\n",
    "\n",
    "producer.flush()\n",
    "print(\"Daten wurden an Kafka gesendet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf65a9-b102-46d5-83d2-53b59f022332",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer( \"amadeus_flights\",\n",
    "bootstrap_servers=\"172.29.16.101:9092\", auto_offset_reset=\"earliest\",\n",
    "value_deserializer=lambda v: json.loads(v.decode(\"utf-8\"))\n",
    ")\n",
    "msg = next(iter(consumer))\n",
    "print(\"Erste empfangene Nachricht:\", msg.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a202dd93-44e3-4bc7-865d-32f927d256ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmadeusFlights\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a1e889f-eb41-4151-8e4e-5a875edf3a5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m kafka_bootstrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m172.29.16.101:9092\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m      2\u001b[0m topic_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamadeus_flights\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m df_kafka \u001b[38;5;241m=\u001b[39m ( \n\u001b[1;32m      5\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkafka_bootstrap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearliest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType() \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetchedAt\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()) \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDestination\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()) \\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinPrice\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType()) \\\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbflug\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType())\n\u001b[1;32m     20\u001b[0m df_json \u001b[38;5;241m=\u001b[39m df_kafka\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(from_json(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m), schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:304\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "kafka_bootstrap = \"172.29.16.101:9092\" \n",
    "topic_name = \"amadeus_flights\"\n",
    "\n",
    "df_kafka = ( \n",
    "    spark.readStream\n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap) \n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    ")\n",
    "\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"FetchedAt\", StringType()) \\\n",
    "    .add(\"Destination\", StringType()) \\\n",
    "    .add(\"MinPrice\", FloatType()) \\\n",
    "    .add(\"Abflug\", StringType())\n",
    "\n",
    "df_json = df_kafka.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "df_fco = df_json.filter(col(\"Destination\") == \"FCO\")\n",
    "\n",
    "df_avg = df_fco.groupBy(\"Abflug\").agg(avg(\"MinPrice\").alias(\"Durchschnittspreis\"))\n",
    "\n",
    "output_path = \"output/amadeus_fco_avg.csv\"\n",
    "query = df_avg.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"output/amadeus_fco_avg/\") \\\n",
    "    .option(\"checkpointLocation\", \"output/checkpoints/\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93b499-9518-40c2-ac40-06e50b526c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_result = pd.read_csv(\"output/amadeus_fco_avg.csv\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(df_spark_result['Abflug'], df_spark_result['Durchschnittspreis'], color='skyblue')\n",
    "plt.ylabel(\"Durchschnittspreis (€)\")\n",
    "plt.title(\"Durchschnittspreise nach FCO – Amadeus (Spark-Auswertung)\")\n",
    "\n",
    "for bar, preis in zip(bars, df_spark_result['Durchschnittspreis']):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f\"{preis:.0f} €\",\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c3320-345f-47e0-b65a-1e2c442a6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = Digraph()\n",
    "dot.node('A', 'Amadeus CSVs (BER, CDG, IST, LHR)')\n",
    "dot.node('B', 'Kafka Producer')\n",
    "dot.node('C', 'Kafka Topic: amadeus_flights')\n",
    "dot.node('D', 'Spark Consumer & ETL')\n",
    "dot.node('E', 'Durchschnittspreise (CSV)')\n",
    "dot.node('F', 'Visualisierung')\n",
    "\n",
    "dot.edges(['AB', 'BC', 'CD', 'DE', 'EF'])\n",
    "dot.render('data_flow_amadeus', format='png', cleanup=False)\n",
    "dot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
