{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2adbe1-7964-4c7b-9c99-64e6d15662dd",
   "metadata": {},
   "source": [
    "# Spark Exercise\n",
    "\n",
    "Apache Spark is an excellent tool for data engineering projects due to its robust ability to process large-scale data efficiently through distributed computing. Spark's in-memory processing capabilities significantly enhance the speed of data operations, making it ideal for handling big data workloads. It supports various data sources and formats, offering versatility in data ingestion and transformation. Additionally, Spark's rich API supports multiple programming languages such as Python, Java, and Scala, catering to diverse developer preferences. Its ecosystem, which includes libraries for SQL, machine learning, and graph processing, provides a comprehensive suite for building complex data pipelines and analytics, making it a powerful and flexible choice for data engineering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b1187-dab5-46ec-be83-556037eb7b20",
   "metadata": {},
   "source": [
    "Use Python, ```pyspark``` and ```pandas``` to explore Apache Spark RDD and DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d57a83-9f00-4b12-861c-640f3187fd42",
   "metadata": {},
   "source": [
    "# Spark RDD\n",
    "\n",
    "Spark RDD (Resilient Distributed Dataset) is a fundamental data structure in Apache Spark that enables fault-tolerant, distributed processing of large datasets across multiple nodes in a cluster. Spark RDDs provide a higher-level abstraction for performing distributed data processing tasks, including both map (transformations) and reduce (aggregations) operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99852f03-87e2-403a-88e9-ca1770ac05da",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5a1162-a23f-40d6-bd6a-6f19cccf6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0408145-2924-4059-8a85-dbd974095d81",
   "metadata": {},
   "source": [
    "## Spark Context and Session\n",
    "Initialize Spark Context and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76eb6765-6776-40d0-8d52-1e832fef01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"LoadClosingOddsRDD\")\n",
    "    .config(\"spark.master\", \"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2597f6-b983-4042-8918-b7c30edba96b",
   "metadata": {},
   "source": [
    "## Load Data into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1272981-d96a-4173-b34b-42e24759b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"closing_odds.csv\"\n",
    "df = pd.read_csv(\n",
    "    csv_path,\n",
    "    compression=\"gzip\",      \n",
    "    encoding=\"utf-8\",        \n",
    ")\n",
    "\n",
    "data_tuples = list(df.itertuples(index=False, name=None))\n",
    "\n",
    "rdd = sc.parallelize(data_tuples)          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261385f-aba5-4063-9185-a1208f21bc14",
   "metadata": {},
   "source": [
    "## Map Operation\n",
    "\n",
    "Split data into individual parts and create key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3774b927-cb7d-4885-8970-864490e0e9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key-value pairs:\n",
      "(170088, ('England: Premier League', '2005-01-01', 'Liverpool', 0, 'Chelsea', 1, 2.9944, 3.1944, 2.2256, 3.2, 3.25, 2.29, 'Paddy Power', 'Sportingbet', 'Expekt', 9, 9, 9))\n",
      "(170089, ('England: Premier League', '2005-01-01', 'Fulham', 3, 'Crystal Palace', 1, 1.9456, 3.2333, 3.6722, 2.04, 3.3, 4.15, 'Pinnacle Sports', 'bet-at-home', 'Expekt', 9, 9, 9))\n",
      "(170090, ('England: Premier League', '2005-01-01', 'Aston Villa', 1, 'Blackburn', 0, 1.8522, 3.2611, 4.0144, 2.0, 3.4, 4.5, 'Pinnacle Sports', 'Paddy Power', 'Sportingbet', 9, 9, 9))\n",
      "(170091, ('England: Premier League', '2005-01-01', 'Bolton', 1, 'West Brom', 1, 1.6122, 3.4133, 5.4722, 1.67, 3.57, 6.27, 'Coral', 'Pinnacle Sports', 'Pinnacle Sports', 9, 9, 9))\n",
      "(170092, ('England: Premier League', '2005-01-01', 'Charlton', 1, 'Arsenal', 3, 5.9878, 3.4778, 1.5567, 7.0, 3.6, 1.62, 'Expekt', 'Paddy Power', 'bet365', 9, 9, 9))\n"
     ]
    }
   ],
   "source": [
    "rdd_kv = rdd.map(lambda tup: (tup[0], tup[1:]))\n",
    "print(\"key-value pairs:\")\n",
    "for kv in rdd_kv.take(5):\n",
    "    print(kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32251e9d-3f2a-4fc2-b9a4-ef5a1f94ea0c",
   "metadata": {},
   "source": [
    "## Reduce Operation\n",
    "\n",
    "Reduce your key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef497dcd-5b23-491b-8e07-73a6fef923df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('England: Premier League', '2005-01-01', 'Aston Villa', 1, 'Blackburn', 0, 1.8522, 3.2611, 4.0144, 2.0, 3.4, 4.5, 'Pinnacle Sports', 'Paddy Power', 'Sportingbet', 9, 9, 9): 1 games\n",
      "('England: Championship', '2005-01-01', 'Derby', 0, 'Cardiff', 1, 1.6875, 3.3625, 4.5188, 1.73, 3.5, 5.5, 'bet365', 'bet365', 'Coral', 8, 8, 8): 1 games\n",
      "('England: Championship', '2005-01-01', 'QPR', 0, 'Brighton', 0, 1.8388, 3.2888, 3.7788, 1.9, 3.66, 4.03, 'Expekt', 'Pinnacle Sports', 'Pinnacle Sports', 8, 8, 8): 1 games\n",
      "('England: Championship', '2005-01-01', 'Sheffield Utd', 0, 'Wigan', 2, 2.25, 3.2188, 2.7888, 2.4, 3.25, 3.0, 'Expekt', 'bet365', 'Ladbrokes', 8, 8, 8): 1 games\n",
      "('England: Championship', '2005-01-01', 'Wolves', 1, 'Plymouth', 1, 1.8713, 3.2625, 3.675, 2.0, 3.4, 4.0, 'Pinnacle Sports', 'Pinnacle Sports', 'Sportingbet', 8, 8, 8): 1 games\n",
      "('England: League One', '2005-01-01', 'Bristol City', 2, 'Peterborough', 0, 1.5378, 3.48, 5.6656, 1.57, 3.6, 6.54, 'bet365', 'bet365', 'Pinnacle Sports', 9, 9, 9): 1 games\n",
      "('England: League One', '2005-01-01', 'Oldham', 2, 'Tranmere', 2, 2.5444, 3.2044, 2.45, 2.65, 3.34, 2.6, 'Expekt', 'Pinnacle Sports', 'Sportingbet', 9, 9, 9): 1 games\n",
      "('England: League Two', '2005-01-01', 'Cambridge Utd', 0, 'Boston', 1, 2.4856, 3.1744, 2.5211, 2.7, 3.25, 2.65, 'Pinnacle Sports', 'Ladbrokes', 'Expekt', 9, 9, 9): 1 games\n",
      "('England: League Two', '2005-01-01', 'Leyton Orient', 2, 'Kidderminster', 1, 1.6178, 3.4567, 4.7778, 1.74, 3.51, 5.15, 'Pinnacle Sports', 'Pinnacle Sports', 'Expekt', 9, 9, 9): 1 games\n",
      "('England: League Two', '2005-01-01', 'Macclesfield', 1, 'Chester', 2, 1.7822, 3.3, 4.0, 1.85, 3.5, 4.34, 'Expekt', 'bet365', 'Pinnacle Sports', 9, 9, 9): 1 games\n"
     ]
    }
   ],
   "source": [
    "matches_per_league_rdd = (\n",
    "    rdd_kv\n",
    "    .map(lambda parts: (parts[1], 1))            \n",
    "    .reduceByKey(lambda a, b: a + b)            \n",
    ")\n",
    "\n",
    "for league, count in matches_per_league_rdd.take(10):\n",
    "    print(f\"{league}: {count} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c597c-a9a2-489f-9399-4be9a5af7e78",
   "metadata": {},
   "source": [
    "## Collect Results\n",
    "\n",
    "Because of lazy evaluation, the map-reduce operation is performed only now. Show what you calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec9af1-eb46-40cd-9d58-352e70599d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_per_league_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724a926-8e29-459f-96d0-51d202ff14cb",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663597f-efd6-4742-a7d5-951c4cc86b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_per_league_rdd \\\n",
    "    .map(lambda kv: f\"{kv[0]},{kv[1]}\") \\\n",
    "    .saveAsTextFile(\"/mnt/data/output/matches_per_league_txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742494d9-e732-420e-8916-11675a687a35",
   "metadata": {},
   "source": [
    "# Spark DataFrame\n",
    "\n",
    "Spark DataFrame is a distributed collection of data organized into named columns, designed for efficient data manipulation and analysis in Apache Spark. It is used for various data processing tasks such as data ingestion, transformation, querying, and analysis in Apache Spark, providing a high-level abstraction that simplifies working with structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099156e-9b93-4b59-a6a7-6dde4eaccb22",
   "metadata": {},
   "source": [
    "## Load Data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2cfa50-5828-48be-924f-a6d5b9b65254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07720bb1-9c9d-4646-be91-371d9a7dd204",
   "metadata": {},
   "source": [
    "## View DataFrame Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01438b-5005-44ab-a57a-6ebccf6f6a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979f855-8774-47a7-8714-dfe2ac19df04",
   "metadata": {},
   "source": [
    "## View DataFrame Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12416e-28d2-45d9-bff7-48b1a24cf181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070f56a-19a3-44cd-a3d2-510c2120bafa",
   "metadata": {},
   "source": [
    "## Filter Data\n",
    "\n",
    "Performe a filter operation on a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e49a3-e847-4d9c-87ed-011ce1cc03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d45665-aa97-4cbf-ba96-88077cf20d30",
   "metadata": {},
   "source": [
    "## Group By and Aggregate\n",
    "\n",
    "Performe a group by and aggregat operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bd2b0a-4bac-4dfe-8160-b27acc7a3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d127b12-9f67-42ba-b6de-de3fcfc2dbb9",
   "metadata": {},
   "source": [
    "## Save DataFrame to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd662d-2a24-475d-a5b5-2a1d2eec6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
